{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d43f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 1: Training on Source Domain ===\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0001_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0001_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0002_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0002_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0003_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0003_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0004_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0004_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0005_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0005_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0006_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0006_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0007_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0007_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0008_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0008_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n",
      "Error loading C:/DCASE_Temp/PreTrained\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0009_pro_A_vel_4_loc_A.npy or C:/DCASE_Temp/FineTuned\\Source\\bearing\\BEATs_aug_section_00_source_train_normal_0009_pro_A_vel_4_loc_A.npy: Unable to allocate 4.38 MiB for an array with shape (1148928,) and data type float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# === Phase 1: Training on Source Domain ===\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Phase 1: Training on Source Domain ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m source_samples = \u001b[43mload_fused_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSource\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(source_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fused samples from Source domain.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Initialize and train the autoencoder\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mload_fused_data\u001b[39m\u001b[34m(domain)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Load both embeddings: each is (1496, 786)\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     emb_frozen = torch.tensor(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_file\u001b[49m\u001b[43m)\u001b[49m, dtype=torch.float32).to(DEVICE)\n\u001b[32m    113\u001b[39m     emb_qlora  = torch.tensor(np.load(q_file), dtype=torch.float32).to(DEVICE)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krishkannan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:488\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    485\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.open_memmap(file, mode=mmap_mode,\n\u001b[32m    486\u001b[39m                                   max_header_size=max_header_size)\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krishkannan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\format.py:836\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m         array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    838\u001b[39m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[32m    839\u001b[39m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    847\u001b[39m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[32m    848\u001b[39m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[32m    849\u001b[39m         array = numpy.ndarray(count, dtype=dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIGURATION --------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBED_DIM = 768\n",
    "LATENT_DIM = 128\n",
    "\n",
    "# Epochs for each phase:\n",
    "SOURCE_EPOCHS = 50\n",
    "TARGET_EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "# Machine list as defined\n",
    "MACHINE_LIST = [\"bearing\", \"fan\", \"gearbox\", \"slider\", \"toycar\", \"toytrain\", \"valve\"]\n",
    "\n",
    "# Folder directories (adjust these base paths as needed)\n",
    "BASE_PRETRAINED = r\"C:/DCASE_Temp/PreTrained\"\n",
    "BASE_FINETUNED = r\"C:/DCASE_Temp/FineTuned\"\n",
    "# Domain subfolders: \"Source\" and \"Target\"\n",
    "DOMAIN_SOURCE = \"Source\"\n",
    "DOMAIN_TARGET = \"Target\"\n",
    "\n",
    "\n",
    "# -------- Gated Fusion Module --------\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.gate_layer = nn.Linear(embedding_dim * 2, embedding_dim)  # 1536 → 768\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, e_frozen, e_qLoRA):\n",
    "        concat = torch.cat([e_frozen, e_qLoRA], dim=-1)  # shape: [T, 1536]\n",
    "        gate = self.sigmoid(self.gate_layer(concat))     # shape: [T, 768]\n",
    "        return gate * e_qLoRA + (1 - gate) * e_frozen     # shape: [T, 768]\n",
    "\n",
    "\n",
    "# -------- Autoencoder (CNN Mahalanobis AE) --------\n",
    "class CNNMahalanobisAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(CNNMahalanobisAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 3, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 3),\n",
    "            nn.Unflatten(1, (64, 4, 3)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,0)), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(1,1)), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(1,0)), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "# -------- UTILITY FUNCTIONS --------\n",
    "def to_img(tensor1d):\n",
    "    \"\"\"\n",
    "    Converts a 1D tensor of size (786,) to image shape (1, 32, 24)\n",
    "    Assumes that 32*24=768 is close enough to 786.\n",
    "    If necessary, you can pad or reduce dimensions accordingly.\n",
    "    Here we perform a simple view; adjust if your data actually needs it.\n",
    "    \"\"\"\n",
    "    # If the input is larger than expected, you might slice it:\n",
    "    tensor1d = tensor1d[:768]\n",
    "    return tensor1d.view(1, 32, 24)\n",
    "\n",
    "def load_fused_data(domain):\n",
    "    \"\"\"\n",
    "    domain: \"Source\" or \"Target\"\n",
    "    This function will traverse all machine folders under the given domain.\n",
    "    It loads each .npy file from the PreTrained base and its corresponding file from the FineTuned base,\n",
    "    fuses them using the GatedFusion module, and converts each time step into an image tensor.\n",
    "    Returns a list of tensors, each of shape (1,32,24)\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    # Determine paths for the given domain:\n",
    "    frozen_root = os.path.join(BASE_PRETRAINED, domain)\n",
    "    qlora_root  = os.path.join(BASE_FINETUNED, domain)\n",
    "\n",
    "    # Iterate over each machine type\n",
    "    for machine in MACHINE_LIST:\n",
    "        frozen_machine_dir = os.path.join(frozen_root, machine)\n",
    "        qlora_machine_dir = os.path.join(qlora_root, machine)\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        if not os.path.exists(frozen_machine_dir) or not os.path.exists(qlora_machine_dir):\n",
    "            print(f\"Warning: Missing directory for machine {machine} in domain {domain}\")\n",
    "            continue\n",
    "\n",
    "        # For each .npy file in the frozen directory:\n",
    "        for f_file in glob.glob(os.path.join(frozen_machine_dir, \"*.npy\")):\n",
    "            # The corresponding qLoRA file is assumed to have the same name\n",
    "            q_file = os.path.join(qlora_machine_dir, os.path.basename(f_file))\n",
    "            if not os.path.exists(q_file):\n",
    "                print(f\"Warning: QLoRA file {q_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Load both embeddings: each is (1496, 786)\n",
    "            try:\n",
    "                emb_frozen = torch.tensor(np.load(f_file), dtype=torch.float32).to(DEVICE)\n",
    "                emb_qlora  = torch.tensor(np.load(q_file), dtype=torch.float32).to(DEVICE)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {f_file} or {q_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Fuse the embeddings for all time steps using the fusion model:\n",
    "            fused = fusion_model(emb_frozen, emb_qlora)  # shape: (1496, 786)\n",
    "            \n",
    "            # For each time step (i.e. each row of 786 features), convert to image tensor:\n",
    "            for timestep in fused:\n",
    "                samples.append(to_img(timestep.cpu()))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def train_autoencoder(model, data, epochs, lr=LR):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # We iterate over each sample in our list.\n",
    "    # (Depending on the number of samples, you might also consider batching.)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for x in tqdm(data, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            x = x.unsqueeze(0).to(DEVICE)  # shape: (1, 32, 24)\n",
    "            recon, _ = model(x)\n",
    "            loss = loss_fn(recon, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] Loss: {avg_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------- MAIN PIPELINE: Phase-wise Training --------\n",
    "# -------- MAIN PIPELINE: Phase-wise Training with Saving --------\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate the fusion model (same across both phases)\n",
    "    fusion_model = GatedFusion(EMBED_DIM).to(DEVICE)\n",
    "    fusion_model.eval()\n",
    "\n",
    "    # === Phase 1: Training on Source Domain ===\n",
    "    print(\"=== Phase 1: Training on Source Domain ===\")\n",
    "    source_samples = load_fused_data(\"Source\")\n",
    "    print(f\"Loaded {len(source_samples)} fused samples from Source domain.\")\n",
    "\n",
    "    # Initialize and train the autoencoder\n",
    "    autoencoder = CNNMahalanobisAE(LATENT_DIM).to(DEVICE)\n",
    "    autoencoder = train_autoencoder(autoencoder, source_samples, epochs=SOURCE_EPOCHS)\n",
    "\n",
    "    # Save the model trained on source\n",
    "    torch.save(autoencoder.state_dict(), \"autoencoder_source.pth\")\n",
    "    print(\"Saved source-trained model to autoencoder_source.pth\\n\")\n",
    "\n",
    "    # === Phase 2: Fine-tuning on Target Domain ===\n",
    "    print(\"=== Phase 2: Fine-tuning on Target Domain ===\")\n",
    "    target_samples = load_fused_data(\"Target\")\n",
    "    print(f\"Loaded {len(target_samples)} fused samples from Target domain.\")\n",
    "\n",
    "    # Re-load the source-trained model\n",
    "    autoencoder = CNNMahalanobisAE(LATENT_DIM).to(DEVICE)\n",
    "    autoencoder.load_state_dict(torch.load(\"autoencoder_source.pth\", map_location=DEVICE))\n",
    "    print(\"Loaded source-trained model weights for fine-tuning.\")\n",
    "\n",
    "    # Fine-tune on target domain\n",
    "    autoencoder = train_autoencoder(autoencoder, target_samples, epochs=TARGET_EPOCHS)\n",
    "\n",
    "    # Save final fine-tuned model\n",
    "    torch.save(autoencoder.state_dict(), \"autoencoder_finetuned.pth\")\n",
    "    print(\"Saved fine-tuned model to autoencoder_finetuned.pth\\n\")\n",
    "\n",
    "    print(\"✅ Training pipeline completed.\")\n",
    "    print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda47f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1: Training on Source Domain ===\n",
      "🔄 Fusing and caching data for Source ...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIGURATION --------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBED_DIM = 768\n",
    "LATENT_DIM = 128\n",
    "\n",
    "SOURCE_EPOCHS = 50\n",
    "TARGET_EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "MACHINE_LIST = [\"bearing\", \"fan\", \"gearbox\", \"slider\", \"toycar\", \"toytrain\", \"valve\"]\n",
    "\n",
    "BASE_PRETRAINED = r\"C:/DCASE_Temp/PreTrained\"\n",
    "BASE_FINETUNED = r\"C:/DCASE_Temp/FineTuned\"\n",
    "DOMAIN_SOURCE = \"Source\"\n",
    "DOMAIN_TARGET = \"Target\"\n",
    "\n",
    "# -------- Gated Fusion --------\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.gate_layer = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, e_frozen, e_qLoRA):\n",
    "        concat = torch.cat([e_frozen, e_qLoRA], dim=-1)\n",
    "        gate = self.sigmoid(self.gate_layer(concat))\n",
    "        return gate * e_qLoRA + (1 - gate) * e_frozen\n",
    "\n",
    "\n",
    "# -------- CNN Mahalanobis Autoencoder --------\n",
    "class CNNMahalanobisAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(CNNMahalanobisAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 3, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 3),\n",
    "            nn.Unflatten(1, (64, 4, 3)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,0)), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(1,1)), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(1,0)), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "# -------- UTILS --------\n",
    "def to_img(tensor1d):\n",
    "    tensor1d = tensor1d[:768]\n",
    "    return tensor1d.view(1, 32, 24)\n",
    "\n",
    "\n",
    "def fuse_and_cache(domain, fusion_model, cache_file):\n",
    "    \"\"\"\n",
    "    Fuses frozen and qLoRA embeddings, converts to image tensors, and saves to disk.\n",
    "    Only done once per domain to avoid recomputing in future runs.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"📦 Loading cached fused data for {domain} from {cache_file}\")\n",
    "        return torch.load(cache_file)\n",
    "\n",
    "    print(f\"🔄 Fusing and caching data for {domain} ...\")\n",
    "    samples = []\n",
    "    frozen_root = os.path.join(BASE_PRETRAINED, domain)\n",
    "    qlora_root  = os.path.join(BASE_FINETUNED, domain)\n",
    "\n",
    "    for machine in MACHINE_LIST:\n",
    "        frozen_machine_dir = os.path.join(frozen_root, machine)\n",
    "        qlora_machine_dir  = os.path.join(qlora_root, machine)\n",
    "\n",
    "        if not os.path.exists(frozen_machine_dir) or not os.path.exists(qlora_machine_dir):\n",
    "            print(f\"⚠️ Missing directory for {machine} in {domain}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for f_file in glob.glob(os.path.join(frozen_machine_dir, \"*.npy\")):\n",
    "            q_file = os.path.join(qlora_machine_dir, os.path.basename(f_file))\n",
    "            if not os.path.exists(q_file):\n",
    "                print(f\"⚠️ Missing qLoRA file: {q_file}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                emb_frozen_np = np.load(f_file, mmap_mode='r')\n",
    "                emb_qlora_np  = np.load(q_file, mmap_mode='r')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {f_file} or {q_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                emb_frozen = torch.tensor(emb_frozen_np, dtype=torch.float32).to(DEVICE)\n",
    "                emb_qlora  = torch.tensor(emb_qlora_np, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    fused = fusion_model(emb_frozen, emb_qlora).cpu()\n",
    "\n",
    "                for timestep in fused:\n",
    "                    samples.append(to_img(timestep))\n",
    "\n",
    "                del emb_frozen, emb_qlora, fused\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"🔥 CUDA error while fusing {f_file}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "    print(f\"✅ Fused {len(samples)} samples from {domain}. Saving to {cache_file}\")\n",
    "    torch.save(samples, cache_file)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def train_autoencoder(model, data, epochs, lr=LR):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for x in tqdm(data, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            x = x.unsqueeze(0).to(DEVICE)\n",
    "            recon, _ = model(x)\n",
    "            loss = loss_fn(recon, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] Loss: {avg_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------- MAIN PIPELINE --------\n",
    "if __name__ == \"__main__\":\n",
    "    fusion_model = GatedFusion(EMBED_DIM).to(DEVICE)\n",
    "    fusion_model.eval()\n",
    "\n",
    "    # === Phase 1: Source Domain ===\n",
    "    print(\"\\n=== Phase 1: Training on Source Domain ===\")\n",
    "    source_cache = \"fused_source.pt\"\n",
    "    source_samples = fuse_and_cache(DOMAIN_SOURCE, fusion_model, source_cache)\n",
    "\n",
    "    autoencoder = CNNMahalanobisAE(LATENT_DIM).to(DEVICE)\n",
    "    autoencoder = train_autoencoder(autoencoder, source_samples, epochs=SOURCE_EPOCHS)\n",
    "\n",
    "    torch.save(autoencoder.state_dict(), \"autoencoder_source.pth\")\n",
    "    print(\"💾 Saved source-trained model to autoencoder_source.pth\")\n",
    "\n",
    "    # === Phase 2: Target Domain ===\n",
    "    print(\"\\n=== Phase 2: Fine-tuning on Target Domain ===\")\n",
    "    target_cache = \"fused_target.pt\"\n",
    "    target_samples = fuse_and_cache(DOMAIN_TARGET, fusion_model, target_cache)\n",
    "\n",
    "    autoencoder = CNNMahalanobisAE(LATENT_DIM).to(DEVICE)\n",
    "    autoencoder.load_state_dict(torch.load(\"autoencoder_source.pth\", map_location=DEVICE))\n",
    "    print(\"📥 Loaded source-trained model for fine-tuning.\")\n",
    "\n",
    "    autoencoder = train_autoencoder(autoencoder, target_samples, epochs=TARGET_EPOCHS)\n",
    "\n",
    "    torch.save(autoencoder.state_dict(), \"autoencoder_finetuned.pth\")\n",
    "    print(\"💾 Saved fine-tuned model to autoencoder_finetuned.pth\")\n",
    "\n",
    "    print(\"✅ All done. Pipeline completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f7a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
