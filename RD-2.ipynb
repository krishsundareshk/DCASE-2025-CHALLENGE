{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CONFIGURATION & IMPORTS\n",
    "# --------------------------\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from BEATs import BEATs, BEATsConfig\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_PATH = \"K:/DCASE/BEATs_iter3.pt\"\n",
    "TRAIN_DIR = \"C:/DCASE_Temp/BEATs/Embeddings\"\n",
    "TEST_DIR = \"K:/DCASE/BEATs/Test_Embeddings\"\n",
    "LABEL_DIR = \"K:/DCASE/labels\"\n",
    "SAVE_DIR = \"K:/DCASE\"\n",
    "MASK_PARAM = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Loading BEATs model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krishkannan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BEATs model ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# LOAD BEATs MODEL\n",
    "# --------------------------\n",
    "print(\"\\nðŸ”„ Loading BEATs model...\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "cfg = BEATsConfig()\n",
    "cfg.input_patch_size = (16, 16)\n",
    "cfg.conv_bias = checkpoint[\"cfg\"].get(\"conv_bias\", False)\n",
    "model = BEATs(cfg)\n",
    "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"âœ… BEATs model ready!\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# SpecAugment Function (Optional)\n",
    "# --------------------------\n",
    "def apply_specaugment(tensor, mask_param=MASK_PARAM):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    tensor = torchaudio.transforms.FrequencyMasking(mask_param)(tensor)\n",
    "    tensor = torchaudio.transforms.TimeMasking(mask_param)(tensor)\n",
    "    return tensor.squeeze(0)\n",
    "\n",
    "# --------------------------\n",
    "# Embedding Extraction\n",
    "# --------------------------\n",
    "def extract_beats_embedding(wav_file, apply_aug=False):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(wav_file)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Loading {wav_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "\n",
    "    waveform = waveform.to(DEVICE)\n",
    "    waveform = torch.nn.functional.pad(waveform, (0, max(0, 16000*30 - waveform.shape[1])))\n",
    "    waveform = waveform[:, :16000*30]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_features(waveform)[0].squeeze(0)\n",
    "        if apply_aug:\n",
    "            features = apply_specaugment(features)\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "# --------------------------\n",
    "# Process WAV files to .npy\n",
    "# --------------------------\n",
    "def process_wavs(input_dir, output_dir, apply_aug=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for machine in os.listdir(input_dir):\n",
    "        in_path = os.path.join(input_dir, machine)\n",
    "        out_path = os.path.join(output_dir, machine)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "        for file in os.listdir(in_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                full_in = os.path.join(in_path, file)\n",
    "                full_out = os.path.join(out_path, f\"BEATs_{'aug_' if apply_aug else ''}{file.replace('.wav','.npy')}\")\n",
    "\n",
    "                if os.path.exists(full_out):\n",
    "                    print(f\"[SKIP] {file}\")\n",
    "                    continue\n",
    "\n",
    "                emb = extract_beats_embedding(full_in, apply_aug)\n",
    "                if emb is not None:\n",
    "                    np.save(full_out, emb)\n",
    "                    print(f\"[SAVE] {full_out}\")\n",
    "\n",
    "# --------------------------\n",
    "# Dataset\n",
    "# --------------------------\n",
    "class BEATsEmbeddingDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.paths = [\n",
    "            os.path.join(dp, f) for dp, _, fs in os.walk(root_dir) for f in fs if f.endswith(\".npy\")\n",
    "        ]\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.paths[idx], mmap_mode='r').astype(np.float32)\n",
    "        return torch.from_numpy(data), self.paths[idx]\n",
    "\n",
    "class TestEmbeddingDataset(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.paths = sorted([os.path.join(dir, f) for f in os.listdir(dir) if f.endswith(\".npy\")])\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.load(self.paths[idx]).astype(np.float32)\n",
    "        return torch.from_numpy(x), os.path.basename(self.paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Student Model\n",
    "# --------------------------\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "    def forward(self, x): return self.linear(x)\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=768):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, out_dim, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "# --------------------------\n",
    "# Cosine Loss\n",
    "# --------------------------\n",
    "def cosine_loss(teacher, student):\n",
    "    t = F.normalize(teacher, dim=-1)\n",
    "    s = F.normalize(student, dim=-1)\n",
    "    return 1 - (t * s).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "def train_model():\n",
    "    dataset = BEATsEmbeddingDataset(TRAIN_DIR)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = nn.Sequential(Bottleneck(), StudentNet()).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            bottleneck = model[0](x)\n",
    "            output = model[1](bottleneck)\n",
    "            loss = cosine_loss(x, output)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.4f} | Time: {time.time()-start:.2f}s\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"student_model_best.pth\"))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"student_model_last.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Evaluation\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "class TestEmbeddingDataset(Dataset):\n",
    "    def __init__(self, machine_dir):\n",
    "        self.paths = sorted([\n",
    "            os.path.join(machine_dir, f)\n",
    "            for f in os.listdir(machine_dir)\n",
    "            if f.endswith(\".npy\")\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        data = np.load(path).astype(np.float32)\n",
    "        filename = os.path.splitext(os.path.basename(path))[0]\n",
    "        return torch.from_numpy(data), filename\n",
    "\n",
    "# Bottleneck Layer (reduce 768 â†’ 256)\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# StudentNet to match Conv1D structure in checkpoint\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 768, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)      # (B, T, 256) â†’ (B, 256, T)\n",
    "        x = self.net(x)\n",
    "        return x.transpose(1, 2)   # (B, 768, T)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model_per_machine(model_checkpoint_path, test_dir, label_dir, output_csv_path=\"all_anomaly_scores.csv\"):\n",
    "    # Load model\n",
    "    model = nn.Sequential(Bottleneck(), StudentNet()).to(device)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for machine_type in sorted(os.listdir(test_dir)):\n",
    "        machine_path = os.path.join(test_dir, machine_type)\n",
    "        label_path = os.path.join(label_dir, f\"{machine_type}.csv\")\n",
    "\n",
    "        if not os.path.isfile(label_path):\n",
    "            print(f\"[WARNING] Label file not found for {machine_type}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        dataset = TestEmbeddingDataset(machine_path)\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        anomaly_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, filename in dataloader:\n",
    "                x = x.to(device)  # (1, T, 768)\n",
    "                student_input = model[0](x)\n",
    "                output = model[1](student_input)\n",
    "\n",
    "                # Cosine similarity\n",
    "                teacher_norm = F.normalize(x, dim=-1)\n",
    "                student_norm = F.normalize(output, dim=-1)\n",
    "                cos_sim = (teacher_norm * student_norm).sum(dim=-1).mean().item()\n",
    "                score = 1 - cos_sim  # Higher score = more anomalous\n",
    "\n",
    "                anomaly_scores.append((filename[0], score))\n",
    "\n",
    "        # Load labels\n",
    "        df_labels = pd.read_csv(label_path)\n",
    "        df_labels[\"filename\"] = df_labels[\"filename\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "        df_scores = pd.DataFrame(anomaly_scores, columns=[\"filename\", \"score\"])\n",
    "\n",
    "        # Merge scores with labels\n",
    "        df = df_scores.merge(df_labels, on=\"filename\", how=\"inner\")\n",
    "\n",
    "        unmatched = len(df_scores) - len(df)\n",
    "        if unmatched > 0:\n",
    "            print(f\"[DEBUG] {unmatched} unmatched filenames in {machine_type}: Sample -> {df_scores[~df_scores['filename'].isin(df['filename'])]['filename'].tolist()[:3]}\")\n",
    "\n",
    "        # Check label variety\n",
    "        if df['label'].nunique() < 2:\n",
    "            print(f\"[{machine_type}] Skipped (insufficient label variety).\")\n",
    "            continue\n",
    "\n",
    "        # Compute AUC and pAUC\n",
    "        auc = roc_auc_score(df[\"label\"], df[\"score\"])\n",
    "        p_auc = roc_auc_score(df[\"label\"], df[\"score\"], max_fpr=0.1)\n",
    "\n",
    "        print(f\"[{machine_type}] AUC: {auc:.4f} | pAUC: {p_auc:.4f}\")\n",
    "\n",
    "        df[\"machine_type\"] = machine_type\n",
    "        df[\"AUC\"] = auc\n",
    "        df[\"pAUC\"] = p_auc\n",
    "        all_results.append(df)\n",
    "\n",
    "    if all_results:\n",
    "        df_all = pd.concat(all_results, ignore_index=True)\n",
    "        df_all.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\n[INFO] All scores saved to: {output_csv_path}\")\n",
    "    else:\n",
    "        print(\"[INFO] No valid evaluations were performed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_bearing] Skipped.\n",
      "[test_fan] Skipped.\n",
      "[test_gearbox] Skipped.\n",
      "[test_slider] Skipped.\n",
      "[test_toycar] Skipped.\n",
      "[test_toytrain] Skipped.\n",
      "[test_valve] Skipped.\n",
      "\n",
      "âœ… All scores saved!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Entry Point\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # To extract embeddings:\n",
    "    # process_wavs(\"K:/DCASE/TestingData\", TEST_DIR, apply_aug=False)\n",
    "    # process_wavs(\"K:/DCASE/TrainingData\", TRAIN_DIR, apply_aug=True)\n",
    "\n",
    "    # To train the model:\n",
    "    # train_model()\n",
    "\n",
    "    # To evaluate:\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_bearing] AUC: 0.4938 | pAUC: 0.5095\n",
      "[test_fan] AUC: 0.5215 | pAUC: 0.5016\n",
      "[test_gearbox] AUC: 0.4716 | pAUC: 0.5037\n",
      "[test_slider] AUC: 0.5768 | pAUC: 0.5079\n",
      "[test_toycar] AUC: 0.5475 | pAUC: 0.5158\n",
      "[test_toytrain] AUC: 0.6298 | pAUC: 0.5121\n",
      "[test_valve] AUC: 0.5818 | pAUC: 0.5363\n",
      "\n",
      "[INFO] All scores saved to: K:/DCASE/all_machine_scores.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_per_machine(\n",
    "    model_checkpoint_path=\"K:/DCASE/student_model_best.pth\",\n",
    "    test_dir=\"K:/DCASE/BEATs/Test_Embeddings\",\n",
    "    label_dir=\"K:/DCASE/generated_labels\",\n",
    "    output_csv_path=\"K:/DCASE/all_machine_scores.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved: K:/DCASE/generated_labels\\test_bearing.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_fan.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_gearbox.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_slider.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_toycar.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_toytrain.csv - 200 samples\n",
      "[INFO] Saved: K:/DCASE/generated_labels\\test_valve.csv - 200 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_labels_from_filenames(test_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for machine_type in os.listdir(test_dir):\n",
    "        machine_path = os.path.join(test_dir, machine_type)\n",
    "        if not os.path.isdir(machine_path):\n",
    "            continue\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for file in os.listdir(machine_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                label = 1 if \"anomaly\" in file.lower() else 0\n",
    "                filename = os.path.splitext(file)[0]  # No extension\n",
    "                data.append((filename, label))\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"filename\", \"label\"])\n",
    "        output_csv_path = os.path.join(output_dir, f\"{machine_type}.csv\")\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"[INFO] Saved: {output_csv_path} - {len(df)} samples\")\n",
    "\n",
    "# Example usage\n",
    "generate_labels_from_filenames(\n",
    "    test_dir=\"K:/DCASE/BEATs/Test_Embeddings\",\n",
    "    output_dir=\"K:/DCASE/generated_labels\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
